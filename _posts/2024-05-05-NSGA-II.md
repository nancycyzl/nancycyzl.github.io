
Paper: (2002) A fast and elitist multiobjective genetic algorithm- NSGA-II

### Drawbacks of original NSGA algorithm

1. complexity $O(MN^3)$ in nondominated sorting, too high
2. non-elitism approach
3. need to specify sharing parameters

### NSGA-II idea

In genetic algorithm, selection of population of next generation is a key step. In single objective optimization, we can just use objective function as the fitness function. But in multi-objective optimization, there is no standard rule to determine which one is better than another one. Therefore, the key of NSGA and NSGA-II lies in sorting solutions to find the best ones as the next generation. As for mutation and crossover, they are similar to single-objective problem.

Innovations: 
1. Original NSGA use nondominated sorting, but the algorithm has high complexity. NSGA-II improves the algorithm to reduce complexity by introducing two things:
	- domination count $n_p$: number of solutions that dominate me
	- set $S_p$: the set of solutions that I dominates
	Smaller domination count is better than larger one, as it is more close to pareto front
2. introduce crowding distance. If two solutions have the same domination count, then use crowding distance to sort them. 
3. constraint handling: extent original dominant test to a constraint-dominant test to deal with infeasible solutions

### Fast nondominated sorting approach

**Step**
1. for each $p$ in the population
	1. initialize all solutions with $n_p=0$, $S_p=\phi$
	2. for q in the population
		1. if $p$ dominates $q$, then add $q$ to $S_p$
		2. else if $q$ dominates $p$, then $n_p += 1$
	3. if $n_p$=0, then $p_{rank}=1$ and add $p$ to $F_1$
2. $i=1$
3. while $F_i \neq \phi$
	1. $Q=\phi$
	2. for each p in $F_i$
		1. for each $q$ in $S_p$
			1. $n_q -= 1$
			2. if $n_q=0$, then $q_{rank}=i+1$, add $q$ to $Q$
	3. $i = i+1$
	4. $F_i = Q$

### Crowding distance assignment

**Step**
1. $l$ = number of solutions in a nondominated set $I$
2. for each $i$, set $distance = 0$
3. for each objective $m$
	1. sort $I$ according to objective $m$
	2. for the first and last element in $I$, set $distance=\infty$
	3. for the middle points
		1. calculate the scaled distance along the objective $m$ dimension and add to $distance$

<img src="/img/post2024/crowding_distance.png" alt="image" width="649">

### Main loop

<img src="/img/post2024/NSGAII_procedure.png" alt="image" width="704">

### Constraint handling

Because solutions can be either possible or impossible. Then we add this possible vs impossible into selection mechanism as well. Just to ensure:
1. if solution $i$ is feasible and $j$ is not, then $i$ better than $j$
2. if solution $i$ and $j$ are both infeasible, then the one with less constraint violation is better
3. if solution $i$ and $j$ are both feasible, then use the normal dominant test to check which one is better

The above mechanism extends original dominant test to a **constraint-dominate** test. And when the problem have constraint, change the dominant test in above algorithm to this constraint-dominate test.