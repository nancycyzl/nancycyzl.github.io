---
layout:       post
title:        "TF-IDF"
author:       "Nancycy"
header-style: text
catalog:      true
mathjax:      true
tags:
    - optimization
---

## Term frequency - inverse document frequency

The TF-IDF (Term Frequency-Inverse Document Frequency) algorithm is a widely used method in information retrieval and text mining to evaluate the importance of a word in a document relative to a collection of documents (or corpus). It combines two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).

Importance concept:
- term $t$: a word or phrase
- document $d$
- corpus $D$: the collection of document (total $N$ documents)

### Term frequency

It measures the frequency of a term in one document. If a terms appears many times in a document, it is more important.

$$TD(t,d) = \frac{number \ of \ term \ t \ in \ d}{number \ of \ all \ terms \ in \ d}$$

#### Variant

**Solve linear increasing problem**

If term appears 100 times in document $A$ and 200 times in document $B$, this does not mean document $B$ is two times more important than $A$ with respect to this term. Such linear increasing problem can be solved by taking the log.

### Inverse document frequency

Simply using term frequency has one problem. Some common words like "a", "the", "and" appear very often, but they carry no meanings. So we want to penalize this kinds of words. These words have one characteristic: common in all documents. So if a term appears in many documents, it is not that important to a single document. In other words, document frequency larger, the term less important to the current document. 

**Document frequency** measures how many documents in which a term has appeared (at least once).

$$DF(t,D) = \frac{number \ of \ documents \ that \ t \ occures}{number \ of \ all \ documents}$$

**Inverse document frequency** measures the importance of a term across the entire corpus, i.e., the inverse of document frequency and take the log.

$$IDF(t,D) = \log\frac{1}{document \ frequency} = \log\frac{number \ of \ all \ documents}{number \ of \ documents \ that \ t \ occures}$$ 

#### Variant

**To solve 0 denominator problem**

It is possible that a term does not appear in the corpus as all, so we can modify the calculation as:

$$IDF(t,D) = \log\frac{number \ of \ all \ documents + 1}{number \ of \ documents \ that \ t \ occures+1} + 1$$ 

### TF-IDF

Finally, we can multiple $term \ frequecy$ by the  $inverse \ document \ frequency$ to account for commonness across all documents.

$$TF-IDF(t,d,D) = TD(t,d)*IDF(t,D)$$

## Implementation

**Using Scikit-learn**

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "What is the weather like today",
    "what is for dinner tonight",
    "this is question worth pondering",
    "it is a beautiful day today"
]

tfidf_vec = TfidfVectorizer()

# 使用 fit_transform() 得到 TF-IDF 矩阵
tfidf_matrix = tfidf_vec.fit_transform(corpus)
print(tfidf_matrix)
# Output example
# (x, y): x is the document id, y is the term id
# (0, 4) 0.332
# (0, 13) 0.433

# 使用 get_feature_names() 得到不重复的单词
print(tfidf_vec.get_feature_names_out())
# Output example
# ["this", "what", "is" ...]

# 得到每个单词对应的 ID
print(tfidf_vec.vocabulary_)
# Output example
# ["What": 14, "weather": 13, "is": 4...]
```

**From scratch**

```python
import math
from collections import Counter

def compute_tf(document):
    tf_dict = {}
    total_terms = len(document)
    term_counts = Counter(document)
    for term, count in term_counts.items():
        tf_dict[term] = count / total_terms
    return tf_dict

def compute_idf(corpus):
    idf_dict = {}
    total_documents = len(corpus)
    all_terms = set(term for document in corpus for term in document)
    for term in all_terms:
        containing_docs = sum(1 for document in corpus if term in document)
        idf_dict[term] = math.log(total_documents / (1 + containing_docs)) + 1  # add 1 to avoid division by zero
    return idf_dict

def compute_tfidf(corpus):
    tfidf_dict = []
    idf_dict = compute_idf(corpus)
    for document in corpus:
        tf_dict = compute_tf(document)
        tfidf_document = {term: tf * idf_dict[term] for term, tf in tf_dict.items()}
        tfidf_dict.append(tfidf_document)
    return tfidf_dict

# Example usage
documents = [
    "this is a sample document".split(),
    "this document is a sample".split(),
    "this is another example example example".split()
]

tfidf_values = compute_tfidf(documents)

# Print the TF-IDF values for each document
for i, doc_tfidf in enumerate(tfidf_values):
    print(f"Document {i+1} TF-IDF:")
    for term, tfidf in doc_tfidf.items():
        print(f"  {term}: {tfidf:.4f}")

```

## References

- https://blog.csdn.net/benzhujie1245com/article/details/118228847
- https://zhuanlan.zhihu.com/p/31197209
- https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/